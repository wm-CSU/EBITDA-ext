{
  "model_type": "roberta",
  "max_seq_len": 512,
  "model_path": "model/chinese-roberta-wwm-ext/",
  "last_model_path": "model/chinese-roberta-wwm-ext/",
  "log_path": "log/",

  "train_file_path": "data/train.csv",
  "valid_file_path": "data/valid.csv",

  "batch_size": 32,
  "multi_gpu": false,
  "use_whitening": true,

  "bert_model_path": "../transformers_PTM/bert-base-chinese/",
  "pooling": "first-last-avg",
  "hidden_size": 768,
  "num_classes": 2,
  "dropout": 0.01,
  "bilstm_num_layers": 1,

  "lr": 3e-5,
  "num_epoch": 50,
  "num_warmup_steps": 2000,
  "gradient_accumulation_steps": 1,
  "max_grad_norm": 1e-3,

  "experiment_name": "RoBERTa-fine-tune-all_data"
}